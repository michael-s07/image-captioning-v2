{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1721541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries that will be used for the project\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import re\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.applications import efficientnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3418eeee",
   "metadata": {},
   "source": [
    "## Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb9faeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    with open(file) as cap_file:\n",
    "        caption = cap_file.readlines()\n",
    "        cap_map = {}\n",
    "        text_data = []\n",
    "        skip_img = set()\n",
    "        \n",
    "    for i in caption:\n",
    "        i = i.rstrip('\\n')\n",
    "        img, cap = i.split('\\t')\n",
    "        \n",
    "        img = img.split(\"#\")[0]\n",
    "        img = os.path.join(\"Flicker8k_Dataset\", img.strip())\n",
    "        \n",
    "        token = cap.strip().split()\n",
    "        if len(token) < 5 or len(token) > 25:\n",
    "            skip_img.add(img)\n",
    "            continue\n",
    "        if img.endswith('jpg') and img not in skip_img:\n",
    "            cap =  \"<start> \" + cap.strip() + \" <end>\"\n",
    "            text_data.append(cap)\n",
    "        if img in cap_map:\n",
    "            cap_map[img].append(cap)\n",
    "        else:\n",
    "            cap_map[img] = [cap]\n",
    "            \n",
    "    for img in skip_img:\n",
    "        if img in cap_map:\n",
    "            del cap_map[img]\n",
    "    return cap_map, text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08479cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val(cap,train_size=0.8, shuffle = True):\n",
    "    all_img = list(cap.keys())\n",
    "    if shuffle:\n",
    "        np.random.shuffle(all_img)\n",
    "        \n",
    "    train_size = int(len(cap) * train_size)\n",
    "    training_data = {img: cap[img] for img in all_img[:train_size]}\n",
    "    validation_data = {img: cap[img] for img in all_img[train_size:]}\n",
    "    \n",
    "    return training_data, validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "905bacd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_map, text_data = load_data(\"Flickr8k_text/Flickr8k.token.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "184ebc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  6115\n",
      "Number of validation samples:  1529\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data = split_train_val(cap_map)\n",
    "print(\"Number of training samples: \", len(train_data))\n",
    "print(\"Number of validation samples: \", len(valid_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20212b4",
   "metadata": {},
   "source": [
    "### Vectorizing the text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e781d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dcc3a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
    "strip_chars = strip_chars.replace(\"<\", \"\")\n",
    "strip_chars = strip_chars.replace(\">\", \"\")\n",
    "\n",
    "vectorization = TextVectorization(\n",
    "    max_tokens=10000,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=25,\n",
    "    standardize=standardization,\n",
    ")\n",
    "vectorization.adapt(text_data)\n",
    "\n",
    "# Data augmentation for image data\n",
    "image_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.2),\n",
    "        layers.RandomContrast(0.3),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20a0ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_and_resize(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299,299))\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66db3a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(img_path, captions):\n",
    "    return decode_and_resize(img_path), vectorization(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dbfe70fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(images, captions):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, captions))\n",
    "    dataset = dataset.shuffle(len(images))\n",
    "    dataset = dataset.map(process_input, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a8ebde06",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = make_dataset(list(train_data.keys()), list(train_data.values()))\n",
    "\n",
    "valid_dataset = make_dataset(list(valid_data.keys()), list(valid_data.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7168ec3b",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dc5e3e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_model():\n",
    "    model = efficientnet.EfficientNetB0(input_shape=(*(299, 299), 3), include_top=False, weights=\"imagenet\")\n",
    "    model.trainable = False\n",
    "    model_out = model.output\n",
    "    model_out = layers.Reshape((-1, model_out.shape[-1]))(model_out)\n",
    "    cnn_model = keras.models.Model(model.input, model_out)\n",
    "    return cnn_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "046bb350",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.0\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.dense_1 = layers.Dense(embed_dim, activation=\"relu\")\n",
    "        \n",
    "    def call(self, inputs, training, mask=None):\n",
    "        inputs = self.layernorm_1(inputs)\n",
    "        inputs = self.dense_1(inputs)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=None,\n",
    "            training=training,\n",
    "        )\n",
    "        out_1 = self.layernorm_2(inputs + attention_output_1)\n",
    "        return out_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3170fb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_tokens = embedded_tokens * self.embed_scale\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e7699799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ff_dim = ff_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "        )\n",
    "        self.ffn_layer_1 = layers.Dense(ff_dim, activation=\"relu\")\n",
    "        self.ffn_layer_2 = layers.Dense(embed_dim)\n",
    "\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "\n",
    "        self.embedding = PositionalEmbedding(\n",
    "            embed_dim=512, sequence_length=25, vocab_size=10000\n",
    "        )\n",
    "        self.out = layers.Dense(10000, activation=\"softmax\")\n",
    "\n",
    "        self.dropout_1 = layers.Dropout(0.3)\n",
    "        self.dropout_2 = layers.Dropout(0.5)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, training, mask=None):\n",
    "        inputs = self.embedding(inputs)\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
    "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
    "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=combined_mask,\n",
    "            training=training,\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "            training=training,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        ffn_out = self.ffn_layer_1(out_2)\n",
    "        ffn_out = self.dropout_1(ffn_out, training=training)\n",
    "        ffn_out = self.ffn_layer_2(ffn_out)\n",
    "\n",
    "        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)\n",
    "        ffn_out = self.dropout_2(ffn_out, training=training)\n",
    "        preds = self.out(ffn_out)\n",
    "        return preds\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7120b5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(keras.Model):\n",
    "    def __init__(\n",
    "        self, cnn_model, encoder, decoder, num_captions_per_image=5, image_aug=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cnn_model = cnn_model\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "        self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n",
    "        self.num_captions_per_image = num_captions_per_image\n",
    "        self.image_aug = image_aug\n",
    "\n",
    "    def calculate_loss(self, y_true, y_pred, mask):\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "        mask = tf.cast(mask, dtype=loss.dtype)\n",
    "        loss *= mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
    "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
    "        accuracy = tf.math.logical_and(mask, accuracy)\n",
    "        accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
    "\n",
    "    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):\n",
    "        encoder_out = self.encoder(img_embed, training=training)\n",
    "        batch_seq_inp = batch_seq[:, :-1]\n",
    "        batch_seq_true = batch_seq[:, 1:]\n",
    "        mask = tf.math.not_equal(batch_seq_true, 0)\n",
    "        batch_seq_pred = self.decoder(\n",
    "            batch_seq_inp, encoder_out, training=training, mask=mask\n",
    "        )\n",
    "        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n",
    "        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n",
    "        return loss, acc\n",
    "\n",
    "    def train_step(self, batch_data):\n",
    "        batch_img, batch_seq = batch_data\n",
    "        batch_loss = 0\n",
    "        batch_acc = 0\n",
    "\n",
    "        if self.image_aug:\n",
    "            batch_img = self.image_aug(batch_img)\n",
    "\n",
    "        # 1. Get image embeddings\n",
    "        img_embed = self.cnn_model(batch_img)\n",
    "\n",
    "        # 2. Pass each of the five captions one by one to the decoder\n",
    "        # along with the encoder outputs and compute the loss as well as accuracy\n",
    "        # for each caption.\n",
    "        for i in range(self.num_captions_per_image):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss, acc = self._compute_caption_loss_and_acc(\n",
    "                    img_embed, batch_seq[:, i, :], training=True\n",
    "                )\n",
    "\n",
    "                # 3. Update loss and accuracy\n",
    "                batch_loss += loss\n",
    "                batch_acc += acc\n",
    "\n",
    "            # 4. Get the list of all the trainable weights\n",
    "            train_vars = (\n",
    "                self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "            )\n",
    "\n",
    "            # 5. Get the gradients\n",
    "            grads = tape.gradient(loss, train_vars)\n",
    "\n",
    "            # 6. Update the trainable weights\n",
    "            self.optimizer.apply_gradients(zip(grads, train_vars))\n",
    "\n",
    "        # 7. Update the trackers\n",
    "        batch_acc /= float(self.num_captions_per_image)\n",
    "        self.loss_tracker.update_state(batch_loss)\n",
    "        self.acc_tracker.update_state(batch_acc)\n",
    "\n",
    "        # 8. Return the loss and accuracy values\n",
    "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
    "\n",
    "    def test_step(self, batch_data):\n",
    "        batch_img, batch_seq = batch_data\n",
    "        batch_loss = 0\n",
    "        batch_acc = 0\n",
    "\n",
    "        # 1. Get image embeddings\n",
    "        img_embed = self.cnn_model(batch_img)\n",
    "\n",
    "        # 2. Pass each of the five captions one by one to the decoder\n",
    "        # along with the encoder outputs and compute the loss as well as accuracy\n",
    "        # for each caption.\n",
    "        for i in range(self.num_captions_per_image):\n",
    "            loss, acc = self._compute_caption_loss_and_acc(\n",
    "                img_embed, batch_seq[:, i, :], training=False\n",
    "            )\n",
    "\n",
    "            # 3. Update batch loss and batch accuracy\n",
    "            batch_loss += loss\n",
    "            batch_acc += acc\n",
    "\n",
    "        batch_acc /= float(self.num_captions_per_image)\n",
    "\n",
    "        # 4. Update the trackers\n",
    "        self.loss_tracker.update_state(batch_loss)\n",
    "        self.acc_tracker.update_state(batch_acc)\n",
    "\n",
    "        # 5. Return the loss and accuracy values\n",
    "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        #dd called automatically.\n",
    "        return [self.loss_tracker, self.acc_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "682cd704",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = get_cnn_model()\n",
    "encoder = TransformerEncoderBlock(embed_dim=512, dense_dim=512, num_heads=1)\n",
    "decoder = TransformerDecoderBlock(embed_dim=512, ff_dim=512, num_heads=2)\n",
    "caption_model = ImageCaptioningModel(\n",
    "    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01809b1",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d1af58c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "cross_entropy = keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction=\"none\"\n",
    ")\n",
    "\n",
    "# EarlyStopping criteria\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "83769864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler for the optimizer\n",
    "class LRSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, post_warmup_learning_rate, warmup_steps):\n",
    "        super().__init__()\n",
    "        self.post_warmup_learning_rate = post_warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        global_step = tf.cast(step, tf.float32)\n",
    "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
    "        warmup_progress = global_step / warmup_steps\n",
    "        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress\n",
    "        return tf.cond(\n",
    "            global_step < warmup_steps,\n",
    "            lambda: warmup_learning_rate,\n",
    "            lambda: self.post_warmup_learning_rate,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1760e6f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imgcap",
   "language": "python",
   "name": "imgcap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
